services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    restart: always
    volumes:
      - /stable_grace/docker-volumes/ollama:/root/.ollama
    environment:
      OLLAMA_NUM_PARALLEL: '1'
    ports:
      - "11434:11434"

  kokoro:
    image: ghcr.io/eduardolat/kokoro-web:latest
    container_name: kokoro
    restart: always
    ports:
      - "3099:3000"
    env_file: kokoro.env

  ramalama:
    image: quay.io/ramalama/ramalama:0.9
    restart: always
    pull_policy: always
    init: true
    ports:
      - "3098:8080"
    devices:
      - /dev/dri
      - /dev/kfd
    environment:
      - HIP_VISIBLE_DEVICES=0
      - HOME=/tmp
    volumes:
      - /home/tortxof/.local/share/ramalama/store/huggingface/ggml-org/gemma-3-4b-it-GGUF/snapshots/sha256-882e8d2db44dc554fb0ea5077cb7e4bc49e7342a1f0da57901c0802ea21a0863/gemma-3-4b-it-Q4_K_M.gguf:/mnt/models/model.file:ro
      - /home/tortxof/.local/share/ramalama/store/huggingface/ggml-org/gemma-3-4b-it-GGUF/snapshots/sha256-882e8d2db44dc554fb0ea5077cb7e4bc49e7342a1f0da57901c0802ea21a0863/mmproj-model-f16.gguf:/mnt/models/mmproj.file:ro
    security_opt:
      - label:disable
      - no-new-privileges
    cap_drop:
      - ALL
    command: >
      /usr/libexec/ramalama/ramalama-serve-core
      llama-server
      --host 0.0.0.0
      --port 8080
      --model /mnt/models/model.file
      --mmproj /mnt/models/mmproj.file
      --no-warmup
      --log-colors
      --alias ramalama/gemma3:4b
      --ctx-size 131072
      --temp 0.8
      --cache-reuse 256
      -ngl 999
      --threads 8
